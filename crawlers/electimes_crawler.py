from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
import requests
from selenium import webdriver # Selenium ê´€ë ¨ import ì£¼ì„ ì²˜ë¦¬
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager
from crawlers.base_crawler import BaseCrawler
from recommenders.article_recommender import ArticleRecommender
import json
import os
import re
import time
import pytz # pytz ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from notion.notion_client import NotionClient
import joblib
from processors.keyword_processor import KeywordProcessor
from ai_update_content import clean_article_content, generate_one_line_summary_with_llm, generate_key_content

class ElectimesCrawler(BaseCrawler):
    # ì „ë ¥ ì‚°ì—… ê´€ë ¨ í‚¤ì›Œë“œ
    KEYWORDS = [
        'ì¬ìƒì—ë„ˆì§€', 'ì „ë ¥ì¤‘ê°œì‚¬ì—…', 'VPP', 'ì „ë ¥ì‹œì¥', 'ESS', 
        'ì¶œë ¥ì œì–´', 'ì¤‘ì•™ê³„ì•½', 'ì €íƒ„ì†Œ ìš©ëŸ‰', 
        'ì¬ìƒì—ë„ˆì§€ì…ì°°', 'ë³´ì¡°ì„œë¹„ìŠ¤', 
        'ì˜ˆë¹„ë ¥ì‹œì¥', 'í•˜í–¥ì˜ˆë¹„ë ¥', 'ê³„í†µí¬í™”',
        'ì „ë ¥ë§',  # ì¶”ê°€ëœ í‚¤ì›Œë“œ
        'ê¸°í›„ì—ë„ˆì§€ë¶€',  # ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì¶”ê°€
        'íƒœì–‘ê´‘', # ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì¶”ê°€
        'ì „ë ¥ê°ë…ì›'  # ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì¶”ê°€
    ]
    
    def __init__(self, notion_client: NotionClient, recommender: Optional[Any] = None):
        super().__init__('ì „ê¸°ì‹ ë¬¸', 'https://www.electimes.com')
        self._source_name = 'ì „ê¸°ì‹ ë¬¸' # Explicitly store source name
        # ëª¨ë“  ì„¹ì…˜ì˜ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•˜ê¸° ìœ„í•´ URL ìˆ˜ì •
        self.list_url = f"{self.base_url}/news/articleList.html?view_type=sm"
        print("ElectimesCrawler initialized (using requests)")
        
        # í¬ë¡¤ë§ ì´ë ¥ íŒŒì¼ ê²½ë¡œ
        self.history_file = 'crawled_articles.json'
        self.crawled_urls = self.load_crawled_urls()
        
        # Selenium ê´€ë ¨ ì´ˆê¸°í™” ì œê±° ë˜ëŠ” ì£¼ì„ ì²˜ë¦¬
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36')
        
        # WebDriverManager ë²„ê·¸ í•´ê²°: ë™ì  ê²½ë¡œ íƒìƒ‰
        def find_chromedriver_path():
            """WebDriverManager ê²°ê³¼ë¥¼ ê²€ì¦í•˜ê³  ì˜¬ë°”ë¥¸ chromedriver ì°¾ê¸°"""
            import os
            import glob
            
            try:
                # 1ë‹¨ê³„: WebDriverManager ì‹œë„
                wdm_path = ChromeDriverManager().install()
                
                # 2ë‹¨ê³„: ê²°ê³¼ ê²€ì¦
                if os.path.exists(wdm_path) and os.access(wdm_path, os.X_OK):
                    # ì˜¬ë°”ë¥¸ íŒŒì¼ì¸ì§€ í™•ì¸ (í¬ê¸° ì²´í¬)
                    if os.path.getsize(wdm_path) > 1000000:  # 1MB ì´ìƒì´ë©´ ì •ìƒì ì¸ chromedriver
                        return wdm_path
                
                # 3ë‹¨ê³„: ê°™ì€ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤ì œ chromedriver ì°¾ê¸°
                wdm_dir = os.path.dirname(wdm_path)
                potential_drivers = glob.glob(os.path.join(wdm_dir, '**/chromedriver*'), recursive=True)
                
                for driver_path in potential_drivers:
                    if (os.path.isfile(driver_path) and 
                        os.access(driver_path, os.X_OK) and 
                        'chromedriver' in os.path.basename(driver_path) and
                        not driver_path.endswith('.chromedriver') and
                        os.path.getsize(driver_path) > 1000000):  # ì‹¤ì œ ì‹¤í–‰íŒŒì¼ì¸ì§€ í™•ì¸
                        return driver_path
                
                # 4ë‹¨ê³„: ì „ì²´ .wdm ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸° (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
                home_dir = os.path.expanduser("~")
                wdm_base = os.path.join(home_dir, ".wdm", "drivers", "chromedriver")
                if os.path.exists(wdm_base):
                    all_drivers = glob.glob(os.path.join(wdm_base, "**/chromedriver"), recursive=True)
                    for driver_path in all_drivers:
                        if (os.path.isfile(driver_path) and 
                            os.access(driver_path, os.X_OK) and
                            os.path.getsize(driver_path) > 1000000):
                            return driver_path
                
                raise Exception("ì˜¬ë°”ë¥¸ chromedriverë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
            except Exception as e:
                raise Exception(f"ChromeDriver ê²½ë¡œ íƒìƒ‰ ì‹¤íŒ¨: {e}")
        
        try:
            driver_path = find_chromedriver_path()
            service = Service(driver_path)
            print(f"ChromeDriver ê²½ë¡œ: {driver_path}")
        except Exception as e:
            print(f"ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
        
        try:
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.wait = WebDriverWait(self.driver, 30)
            print("Chromium WebDriver initialized successfully")
        except Exception as e:
            print(f"Error initializing Chromium WebDriver: {str(e)}")
            # Selenium ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ requests ì‚¬ìš©ìœ¼ë¡œ í´ë°± (ì„ íƒ ì‚¬í•­)
            print("Falling back to requests for page content.")
            self.driver = None
            
        # AI ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.article_recommender = ArticleRecommender(notion_client)
        self.ai_recommender = recommender  # recommenderë¥¼ ai_recommenderë¡œ ì‚¬ìš©
        self.patterns = None  # ê´€ì‹¬ íŒ¨í„´
        self.keywords = set()  # ë™ì  í‚¤ì›Œë“œ ì„¸íŠ¸
        self.update_crawling_criteria()  # ì´ˆê¸° í¬ë¡¤ë§ ê¸°ì¤€ ì„¤ì •

    def __del__(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if hasattr(self, 'driver') and self.driver is not None:
            try:
                self.driver.quit()
            except Exception as e:
                print(f"Error closing WebDriver: {str(e)}")

    def load_crawled_urls(self) -> set:
        """ì´ì „ì— í¬ë¡¤ë§í•œ URL ëª©ë¡ì„ ë¡œë“œ"""
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
            except Exception as e:
                print(f"Error loading crawled URLs: {str(e)}")
        return set()

    def save_crawled_url(self, url: str):
        """í¬ë¡¤ë§í•œ URLì„ ì €ì¥"""
        self.crawled_urls.add(url)
        try:
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.crawled_urls), f)
        except Exception as e:
            print(f"Error saving crawled URL: {str(e)}")

    def is_recent_article(self, date: datetime) -> bool:
        """ê¸°ì‚¬ê°€ í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ìµœê·¼ 3ì¼ ë‚´ì˜ ê²ƒì¸ì§€ í™•ì¸ (ë‚ ì§œë§Œ ë¹„êµ)"""
        kst = pytz.timezone('Asia/Seoul')
        now_kst = datetime.now(kst) # í•œêµ­ ì‹œê°„ í˜„ì¬ ì‹œê°
        today_kst = now_kst.date() # í•œêµ­ ì‹œê°„ ì˜¤ëŠ˜ ë‚ ì§œ
        three_days_ago_kst = today_kst - timedelta(days=3) # í•œêµ­ ì‹œê°„ 3ì¼ ì „ ë‚ ì§œ

        # ê¸°ì‚¬ ë‚ ì§œì˜ ì‹œê°„ ì •ë³´ ì œê±° (naive datetime ê°€ì •)
        article_date = date.date()

        is_recent = article_date >= three_days_ago_kst
        # print(f"Date check (KST) - Article date (date only): {article_date}, 3 days ago (KST date only): {three_days_ago_kst}, Is recent: {is_recent})") # Debug log ì œê±°
        return is_recent

    def _parse_date_safely(self, date_str: str) -> Optional[datetime]:
        """
        ğŸ”§ ì•ˆì „í•œ ë‚ ì§œ íŒŒì‹± ë©”ì„œë“œ
        
        **ëª©ì **: ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ ì‹œë„í•˜ë˜, ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
        **ì¥ì **: íŒŒì‹± ì‹¤íŒ¨í•œ ê¸°ì‚¬ë¥¼ ëª…í™•íˆ êµ¬ë¶„ ê°€ëŠ¥
        **ë³´ì•ˆ**: í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ëŒ€ì²´í•˜ì§€ ì•Šì•„ í•„í„°ë§ ë¬´ë ¥í™” ë°©ì§€
        
        Args:
            date_str (str): íŒŒì‹±í•  ë‚ ì§œ ë¬¸ìì—´
            
        Returns:
            Optional[datetime]: ì„±ê³µì‹œ datetime ê°ì²´, ì‹¤íŒ¨ì‹œ None
        """
        if not date_str or not date_str.strip():
            return None
            
        date_str = date_str.strip()
        
        # ğŸ¯ ì‹œë„í•  ë‚ ì§œ í˜•ì‹ íŒ¨í„´ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        date_patterns = [
            '%Y.%m.%d %H:%M',     # 2025.06.03 10:00
            '%Y.%m.%d',           # 2025.06.03
            '%Y-%m-%d %H:%M:%S',  # 2025-06-03 10:00:00
            '%Y-%m-%d %H:%M',     # 2025-06-03 10:00
            '%Y-%m-%d',           # 2025-06-03
            '%Y/%m/%d %H:%M',     # 2025/06/03 10:00
            '%Y/%m/%d',           # 2025/06/03
            '%m.%d %H:%M',        # 06.03 10:00 (ì—°ë„ ì—†ìŒ)
            '%m-%d %H:%M',        # 06-03 10:00 (ì—°ë„ ì—†ìŒ)
            '%m/%d %H:%M',        # 06/03 10:00 (ì—°ë„ ì—†ìŒ)
        ]
        
        for pattern in date_patterns:
            try:
                parsed_date = datetime.strptime(date_str, pattern)
                
                # ğŸ” ì—°ë„ê°€ ì—†ëŠ” íŒ¨í„´ì˜ ê²½ìš° í˜„ì¬ ì—°ë„ ì ìš©
                if '%Y' not in pattern:
                    current_year = datetime.now().year
                    parsed_date = parsed_date.replace(year=current_year)
                
                print(f"[Electimes] ë‚ ì§œ íŒŒì‹± ì„±ê³µ: '{date_str}' â†’ {parsed_date} (íŒ¨í„´: {pattern})")
                return parsed_date
                
            except ValueError:
                continue  # ë‹¤ìŒ íŒ¨í„´ ì‹œë„
        
        # ğŸš¨ ëª¨ë“  íŒ¨í„´ ì‹¤íŒ¨ ì‹œ
        print(f"[Electimes] âš ï¸  ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: '{date_str}' - ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹")
        print(f"[Electimes] ğŸ“‹ ì§€ì› í˜•ì‹: {', '.join(date_patterns[:5])} ë“±")
        return None  # âœ… í˜„ì¬ ì‹œê°„ ëŒ€ì‹  None ë°˜í™˜

    def _smart_retry(self, operation_name: str, operation_func, max_retries: int = 3, base_delay: float = 2.0):
        """
        ğŸ”§ ìŠ¤ë§ˆíŠ¸ ë„¤íŠ¸ì›Œí¬ ì¬ì‹œë„ ì‹œìŠ¤í…œ
        
        **íŠ¹ì§•**:
        - ì§€ìˆ˜ ë°±ì˜¤í”„ (Exponential Backoff): 2ì´ˆ â†’ 4ì´ˆ â†’ 8ì´ˆ
        - ì˜¤ë¥˜ ë¶„ë¥˜: ì¼ì‹œì  vs ì˜êµ¬ì  ì˜¤ë¥˜ êµ¬ë¶„
        - ì§€í„°(Jitter): ëœë¤ ìš”ì†Œ ì¶”ê°€ë¡œ ì„œë²„ ë¶€í•˜ ë¶„ì‚°
        
        Args:
            operation_name (str): ì‘ì—… ì´ë¦„ (ë¡œê·¸ìš©)
            operation_func (callable): ì‹¤í–‰í•  í•¨ìˆ˜
            max_retries (int): ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            base_delay (float): ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„(ì´ˆ)
            
        Returns:
            operation_funcì˜ ê²°ê³¼ ë˜ëŠ” None
        """
        import random
        
        # ğŸ” ì¬ì‹œë„ ê°€ëŠ¥í•œ ì˜¤ë¥˜ íƒ€ì…ë“¤
        retryable_exceptions = (
            requests.exceptions.ConnectionError,     # ì—°ê²° ì˜¤ë¥˜
            requests.exceptions.Timeout,            # íƒ€ì„ì•„ì›ƒ
            requests.exceptions.HTTPError,          # HTTP 5xx ì˜¤ë¥˜
            TimeoutException,                       # Selenium íƒ€ì„ì•„ì›ƒ
        )
        
        for attempt in range(max_retries):
            try:
                print(f"[Electimes] {operation_name} ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})")
                result = operation_func()
                print(f"[Electimes] âœ… {operation_name} ì„±ê³µ!")
                return result
                
            except retryable_exceptions as e:
                if attempt < max_retries - 1:
                    # ğŸ¯ ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„° ê³„ì‚°
                    delay = base_delay * (2 ** attempt)  # 2ì´ˆ â†’ 4ì´ˆ â†’ 8ì´ˆ
                    jitter = random.uniform(0.1, 0.3) * delay  # 10-30% ëœë¤ ì¶”ê°€
                    total_delay = delay + jitter
                    
                    print(f"[Electimes] âš ï¸ {operation_name} ì¼ì‹œì  ì˜¤ë¥˜: {type(e).__name__}")
                    print(f"[Electimes] ğŸ”„ {total_delay:.1f}ì´ˆ í›„ ì¬ì‹œë„... (ì§€ìˆ˜ ë°±ì˜¤í”„)")
                    time.sleep(total_delay)
                else:
                    print(f"[Electimes] âŒ {operation_name} ìµœì¢… ì‹¤íŒ¨: {str(e)}")
                    break
                    
            except (requests.exceptions.HTTPError, ValueError, AttributeError) as e:
                # ğŸš« ì˜êµ¬ì  ì˜¤ë¥˜ - ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
                print(f"[Electimes] âŒ {operation_name} ì˜êµ¬ì  ì˜¤ë¥˜ (ì¬ì‹œë„ ì•ˆí•¨): {type(e).__name__} - {str(e)}")
                break
                
            except Exception as e:
                # ğŸ¤” ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ - ë³´ìˆ˜ì ìœ¼ë¡œ ì¬ì‹œë„
                if attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt)
                    print(f"[Electimes] â“ {operation_name} ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {type(e).__name__} - {str(e)}")
                    print(f"[Electimes] ğŸ”„ {delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(delay)
                else:
                    print(f"[Electimes] âŒ {operation_name} ìµœì¢… ì‹¤íŒ¨ (ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜): {str(e)}")
                    break
        
        return None  # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨

    def contains_keywords_and_extract(self, text: str) -> tuple[bool, list]:
        """í…ìŠ¤íŠ¸ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë§¤ì¹­ëœ í‚¤ì›Œë“œ ë°˜í™˜"""
        found_keywords = [keyword for keyword in self.KEYWORDS if keyword in text]
        if found_keywords:
            return True, found_keywords
        return False, []
    
    def contains_keywords(self, text: str) -> bool:
        """ì´ì „ ë²„ì „ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
        contains, _ = self.contains_keywords_and_extract(text)
        return contains

    def get_page_content(self, url: str) -> str:
        """ğŸ”§ ê°œì„ ëœ Selenium í˜ì´ì§€ ì»¨í…ì¸  ê°€ì ¸ì˜¤ê¸° (ìŠ¤ë§ˆíŠ¸ ì¬ì‹œë„ ì ìš©)"""
        if self.driver is None:
            print("Selenium driver not initialized. Cannot get page content.")
            return ""

        def fetch_with_selenium():
            """Seleniumìœ¼ë¡œ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‘ì—…"""
            self.driver.get(url)
            
            # Simple wait for page content to potentially load dynamically
            time.sleep(5)
            
            html_content = self.driver.page_source

            # Save HTML content to file for debugging
            filename = url.split('/')[-1].split('?')[0]
            if not filename:
                filename = 'index.html'
            if not filename.endswith('.html'):
                filename = filename + '.html'
                
            # Append query parameters to filename for distinction
            query_string = url.split('?')
            if len(query_string) > 1:
                filename = f"{filename}?{'?'.join(query_string[1:])}"

            # Replace invalid characters in filename
            filename = re.sub(r'[^a-zA-Z0-9_.-?=&]', '_', filename)
            debug_file = f"debug_{filename}"

            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"Saved HTML content to {debug_file}")
            
            return html_content

        # ğŸš€ ìŠ¤ë§ˆíŠ¸ ì¬ì‹œë„ ì‹œìŠ¤í…œ ì‚¬ìš© (Selenium ì „ìš© ì„¤ì •)
        result = self._smart_retry(
            operation_name=f"Selenium í˜ì´ì§€ ë¡œë“œ ({url})",
            operation_func=fetch_with_selenium,
            max_retries=3,
            base_delay=3.0  # Seleniumì€ ë” ëŠë¦¬ë¯€ë¡œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        )

        if result:
            print(f"Successfully fetched {url} using Selenium. Content length: {len(result)}")
            return result
        else:
            print(f"Failed to fetch {url} using Selenium after all retries")
            return ""

    def parse_html(self, html_content: str) -> BeautifulSoup:
        """Parse HTML content using BeautifulSoup"""
        return BeautifulSoup(html_content, 'html.parser')

    def update_crawling_criteria(self):
        """í¬ë¡¤ë§ ê¸°ì¤€ ì—…ë°ì´íŠ¸"""
        try:
            # AI ì¶”ì²œ ëª¨ë¸ ë° ë²¡í„°ë¼ì´ì € ë¡œë“œ
            model_path = 'feedback/ai_recommend_model.joblib'
            vectorizer_path = 'feedback/ai_recommend_vectorizer.joblib'

            if os.path.exists(model_path) and os.path.exists(vectorizer_path):
                try:
                    self.ai_recommender = joblib.load(model_path)
                    self.vectorizer = joblib.load(vectorizer_path) # Add vectorizer loading
                    print("[Crawler] AI ì¶”ì²œ ëª¨ë¸ ë° ë²¡í„°ë¼ì´ì € ë¡œë“œ ì™„ë£Œ")
                except Exception as model_e:
                    print(f"[Crawler] AI ì¶”ì²œ ëª¨ë¸/ë²¡í„°ë¼ì´ì € ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(model_e)}")
                    self.ai_recommender = None # Ensure it's None on error
                    self.vectorizer = None # Ensure it's None on error
            else:
                print("[Crawler] AI ì¶”ì²œ ëª¨ë¸ ë˜ëŠ” ë²¡í„°ë¼ì´ì € íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. AI ì¶”ì²œ í•„í„°ë§ì„ ê±´ë„ˆëœ€.")
                self.ai_recommender = None
                self.vectorizer = None

            # ê´€ì‹¬ ê¸°ì‚¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            # Note: This part assumes ArticleRecommender can load patterns/keywords without a pre-trained model
            # If not, this needs adjustment.
            if hasattr(self, 'article_recommender') and self.article_recommender:
                 interested_articles = self.article_recommender.notion.get_interested_articles()
                 if interested_articles:
                     # Assuming analyze_article_patterns extracts keywords from interested articles
                     patterns = self.article_recommender.analyze_article_patterns(interested_articles) # patterns ë³€ìˆ˜ ì¶”ê°€
                     if patterns and 'keywords' in patterns:
                         # self.keywordsë¥¼ KEYWORDSì™€ í•©ì¹˜ëŠ” ë¡œì§ í•„ìš” ì‹œ ì¶”ê°€
                         # í˜„ì¬ëŠ” KEYWORDS + ê´€ì‹¬ê¸°ì‚¬ í‚¤ì›Œë“œ ëª¨ë‘ contains_keywordsì—ì„œ ì‚¬ìš©í•˜ë„ë¡ ê°€ì •
                         # self.keywords.update(patterns['keywords'])
                         print(f"[Crawler] ê´€ì‹¬ ê¸°ì‚¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ (contains_keywordsì—ì„œ ì‚¬ìš©ë¨)")
                     else:
                         print("[Crawler] ê´€ì‹¬ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œí•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                 else:
                      print("[Crawler] ê´€ì‹¬ ê¸°ì‚¬ê°€ ì—†ì–´ í‚¤ì›Œë“œ ì¶”ì¶œì„ ê±´ë„ˆëœ€.")
            # else:
            #     print("[Crawler] ArticleRecommenderê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ê´€ì‹¬ ê¸°ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œì„ ê±´ë„ˆëœ€.") # Debug log

        except Exception as e:
            print(f"[Crawler] í¬ë¡¤ë§ ê¸°ì¤€ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            print(traceback.format_exc())

    def _fetch_articles(self, page: int = 1) -> List[Dict[str, Any]]:
        """íŠ¹ì • í˜ì´ì§€ì˜ ê¸°ì‚¬ ëª©ë¡ ì›ì‹œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        url = f"{self.list_url}&page={page}"
        print(f"[Electimes] ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘ (í˜ì´ì§€ {page}): {url}")
        
        try:
            # Selenium ë“œë¼ì´ë²„ê°€ ì´ˆê¸°í™”ë˜ì—ˆìœ¼ë©´ Selenium ì‚¬ìš©, ì•„ë‹ˆë©´ requests ì‚¬ìš©
            if self.driver:
                html_content = self.get_page_content(url)
            else:
                # ğŸš€ ê°œì„ ëœ requests í˜¸ì¶œ (ìŠ¤ë§ˆíŠ¸ ì¬ì‹œë„ ì ìš©)
                def fetch_page_list():
                    """í˜ì´ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‘ì—…"""
                    response = requests.get(url, timeout=15)
                    response.raise_for_status()
                    return response.text

                print(f"[Electimes] Selenium ë“œë¼ì´ë²„ ì‚¬ìš© ë¶ˆê°€. ìŠ¤ë§ˆíŠ¸ ì¬ì‹œë„ë¡œ ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘: {url}")
                html_content = self._smart_retry(
                    operation_name=f"ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ {page})",
                    operation_func=fetch_page_list,
                    max_retries=3,
                    base_delay=1.5  # ëª©ë¡ í˜ì´ì§€ëŠ” ì¢€ ë” ë¹ ë¥´ê²Œ
                )

            if not html_content:
                print(f"[Electimes] í˜ì´ì§€ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {url}")
                return []
                
            soup = self.parse_html(html_content)
            # ìˆ˜ì •ëœ CSS ì„ íƒì ì‚¬ìš©
            articles = []
            # section#section-list li.item ì•„ë˜ì—ì„œ ê¸°ì‚¬ í•­ëª© ì°¾ê¸°
            article_items = soup.select('section#section-list li.item')
            
            if not article_items:
                 print(f"[Electimes] í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì„ íƒì: section#section-list li.item)")
                 # ë¹ˆ ëª©ë¡ì„ ë°˜í™˜í•˜ì—¬ í˜ì´ì§€ë„¤ì´ì…˜ ì¤‘ë‹¨ ë¡œì§ì´ ì‘ë™í•˜ë„ë¡ ìœ ë„
                 return []

            print(f"[Electimes] í˜ì´ì§€ {page}ì—ì„œ {len(article_items)}ê°œì˜ ê¸°ì‚¬ í•­ëª© ë°œê²¬.")

            for item in article_items:
                # ìˆ˜ì •ëœ CSS ì„ íƒì ì‚¬ìš©: ì œëª© ë§í¬, ë‚ ì§œ
                title_link = item.select_one('h4.titles a.linked')
                date_tag = item.select_one('em.replace-date')
                source_tag = item.select_one('span.byline a') # ì¶œì²˜(ê¸°ì ì´ë¦„/ì†Œì†) ê°€ì ¸ì˜¤ê¸° ì¶”ê°€

                title = title_link.text.strip() if title_link else None
                url = self.base_url + title_link['href'] if title_link and title_link.has_attr('href') else None
                date_str = date_tag.text.strip() if date_tag else None
                source = source_tag.text.strip() if source_tag else self._source_name # ì¶œì²˜ ê°€ì ¸ì˜¤ê¸°

                # ğŸ”§ ê°œì„ ëœ ë‚ ì§œ íŒŒì‹± ì‹œë„ (ë‹¤ì–‘í•œ í˜•ì‹ ê³ ë ¤)
                published_date = None
                if date_str:
                    published_date = self._parse_date_safely(date_str)
                            
                # âœ… ì œëª©, URL, ë‚ ì§œê°€ ëª¨ë‘ ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ëœ ê²½ìš°ë§Œ ì¶”ê°€
                if title and url and published_date:
                     articles.append({
                         'title': title,
                         'url': url,
                         'published_date': published_date,
                         'source': source, # ì¶œì²˜ ì¶”ê°€
                         'content': '', # contentëŠ” ìƒì„¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜´
                         'keywords': [], # í‚¤ì›Œë“œëŠ” ë‚˜ì¤‘ì— ì¶”ê°€
                         'ai_recommend': False # AI ì¶”ì²œ ì´ˆê¸°ê°’
                     })
                     # print(f"[Electimes] ê¸°ì‚¬ í•­ëª© ì¶”ì¶œ: {title} - {date_str}") # Debug log ì œê±°
                elif date_str and not published_date:
                    # ğŸ“ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•œ ê¸°ì‚¬ëŠ” ë¡œê·¸ë¡œ ê¸°ë¡
                    print(f"[Electimes] âš ï¸  ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ì‚¬ ì œì™¸: '{title}' - ë‚ ì§œ: '{date_str}'")
                # else:
                #     print(f"[Electimes] ê¸°ì‚¬ í•­ëª© ìŠ¤í‚µ (ì •ë³´ ëˆ„ë½): ì œëª©={title}, URL={url}, ë‚ ì§œ={date_str}") # Debug log ì œê±°
                
            return articles
            
        except requests.exceptions.RequestException as e:
             print(f"[Electimes] HTTP ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ (í˜ì´ì§€ {page}): {str(e)}")
             return []
        except Exception as e:
            print(f"[Electimes] ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ (í˜ì´ì§€ {page}): {str(e)}")
            import traceback
            print(traceback.format_exc())
            return []

    def _extract_date(self, article: Dict[str, Any]) -> Optional[datetime]:
         """ğŸ”§ ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬ì—ì„œ ë‚ ì§œ ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
         date = article.get('published_date')
         if isinstance(date, str):
             # ğŸ”§ ì•ˆì „í•œ ë‚ ì§œ íŒŒì‹± ë©”ì„œë“œ ì‚¬ìš©
             return self._parse_date_safely(date)
         return date

    def crawl(self) -> list:
        """
        ì „ê¸°ì‹ ë¬¸ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        í•œêµ­ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìµœê·¼ 3ì¼ ì´ë‚´ ê¸°ì‚¬ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ í˜ì´ì§€ë¥¼ íƒìƒ‰í•˜ë©°,
        í‚¤ì›Œë“œ ë° AI ì¶”ì²œ í•„í„°ë§ì„ ê±°ì³ ìƒì„¸ ë‚´ìš©ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        """
        print("[Electimes] í¬ë¡¤ë§ ì‹œì‘...")
        self.update_crawling_criteria() # ìµœì‹  ê¸°ì¤€ ì—…ë°ì´íŠ¸
        
        crawled_articles_details = []
        page = 1
        recent_article_found_on_page = True # í˜„ì¬ í˜ì´ì§€ì—ì„œ ìµœê·¼ ê¸°ì‚¬ ë°œê²¬ ì—¬ë¶€
        consecutive_pages_without_recent = 0 # ìµœê·¼ ê¸°ì‚¬ ì—†ëŠ” ì—°ì† í˜ì´ì§€ ìˆ˜
        max_consecutive_without_recent = 3 # ìµœê·¼ ê¸°ì‚¬ ì—†ì´ íƒìƒ‰í•  ìµœëŒ€ ì—°ì† í˜ì´ì§€ ìˆ˜
        
        while recent_article_found_on_page and page <= 20: # ìµœëŒ€ 20í˜ì´ì§€ê¹Œì§€ íƒìƒ‰ (ì•ˆì „ ì¥ì¹˜)
            print(f"[Electimes] í˜ì´ì§€ {page} íƒìƒ‰ ì¤‘...")
            recent_article_found_on_page = False # ë‹¤ìŒ í˜ì´ì§€ë¥¼ ìœ„í•´ ì´ˆê¸°í™”

            raw_articles_on_page = self._fetch_articles(page)
            
            if not raw_articles_on_page:
                 print(f"[Electimes] í˜ì´ì§€ {page}ì—ì„œ ê°€ì ¸ì˜¨ ê¸°ì‚¬ ì—†ìŒ. íƒìƒ‰ ì¢…ë£Œ.")
                 break # ê¸°ì‚¬ ëª©ë¡ì´ ì—†ìœ¼ë©´ íƒìƒ‰ ì¢…ë£Œ

            # 1. ë‚ ì§œ í•„í„°ë§ ë° ìµœê·¼ ê¸°ì‚¬ ë°œê²¬ ì—¬ë¶€ ì²´í¬
            recent_articles_on_page = []
            for article in raw_articles_on_page:
                 published_date = self._extract_date(article)
                 if published_date and self.is_recent_article(published_date):
                     recent_articles_on_page.append(article)
                     recent_article_found_on_page = True # í˜„ì¬ í˜ì´ì§€ì—ì„œ ìµœê·¼ ê¸°ì‚¬ ë°œê²¬!

            if recent_article_found_on_page:
                 consecutive_pages_without_recent = 0 # ìµœê·¼ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìœ¼ë¯€ë¡œ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                 print(f"[Electimes] í˜ì´ì§€ {page}ì—ì„œ ìµœê·¼ 3ì¼ ì´ë‚´ ê¸°ì‚¬ ë°œê²¬. íƒìƒ‰ ê³„ì†.")
            else:
                 consecutive_pages_without_recent += 1
                 print(f"[Electimes] í˜ì´ì§€ {page}ì—ì„œ ìµœê·¼ 3ì¼ ì´ë‚´ ê¸°ì‚¬ ì—†ìŒ. ì—°ì† {consecutive_pages_without_recent} í˜ì´ì§€.")
                 if consecutive_pages_without_recent >= max_consecutive_without_recent:
                     print(f"[Electimes] ìµœê·¼ ê¸°ì‚¬ ì—†ëŠ” í˜ì´ì§€ê°€ {max_consecutive_without_recent}ë²ˆ ì—°ì†ë˜ì–´ íƒìƒ‰ ì¢…ë£Œ.")
                     break # ì—°ì† 3í˜ì´ì§€ ë™ì•ˆ ìµœê·¼ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ íƒìƒ‰ ì¢…ë£Œ

            # ë‚ ì§œ í•„í„°ë§ í†µê³¼í•œ ê¸°ì‚¬ë“¤ì— ëŒ€í•´ì„œë§Œ ì¶”ê°€ ì²˜ë¦¬
            articles_to_process = recent_articles_on_page
            
            # 2. AI ì¶”ì²œ ì˜ˆì¸¡ ì ìš© (ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš°ì—ë§Œ)
            if self.ai_recommender and self.vectorizer and articles_to_process:
                # predict_ai_recommendëŠ” ArticleRecommender ì¸ìŠ¤í„´ìŠ¤ ë©”ì„œë“œì´ë¯€ë¡œ self.article_recommender ì‚¬ìš©
                # í•˜ì§€ë§Œ predict_ai_recommendëŠ” ë‚´ë¶€ì ìœ¼ë¡œ vectorizerì™€ modelì„ ë¡œë“œí•¨. ì¤‘ë³µ ë¡œë”© ë°©ì§€ ë˜ëŠ” ì¸ì ì „ë‹¬ í•„ìš”.
                # í˜„ì¬ ai_recommender ì¸ìŠ¤í„´ìŠ¤ëŠ” LogisticRegression ëª¨ë¸ ê°ì²´ì´ë¯€ë¡œ predict ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ì•¼ í•¨.
                # ê·¸ë¦¬ê³  predictì—ëŠ” ë²¡í„°í™”ëœ í…ìŠ¤íŠ¸ ë°ì´í„°(X_vec)ê°€ í•„ìš”í•˜ë¯€ë¡œ vectorizerë¥¼ ì‚¬ìš©í•´ì•¼ í•¨.
                
                try:
                     # í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì œëª© + ë³¸ë¬¸. ë³¸ë¬¸ì€ get_article_content í›„ ì•Œ ìˆ˜ ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ëª©ë¡ ì •ë³´ë§Œ ì‚¬ìš©)
                     # AI ì˜ˆì¸¡ì€ ìƒì„¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¨ í›„ í•´ì•¼ ë” ì •í™•í•˜ì§€ë§Œ, í˜„ì¬ êµ¬ì¡°ìƒ ëª©ë¡ ë‹¨ê³„ì—ì„œ ì¼ë¶€ ì˜ˆì¸¡ ì‹œë„.
                     # ìƒì„¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¨ í›„ AI ì˜ˆì¸¡ ë° í•„í„°ë§í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì´ í•©ë¦¬ì ì…ë‹ˆë‹¤.
                     # ê³„íšì„ ìˆ˜ì •í•˜ì—¬, í˜ì´ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° -> ë‚ ì§œ í•„í„°ë§ -> ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ (ì¡°ê±´ë¶€) -> ìƒì„¸ ë‚´ìš©ì— ëŒ€í•´ AI ì˜ˆì¸¡ -> ìµœì¢… í•„í„°ë§ ìˆœì„œë¡œ ë³€ê²½í•©ë‹ˆë‹¤.

                     # **ìˆ˜ì •ëœ ê³„íš ì ìš©:**
                     # í˜ì´ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° -> ë‚ ì§œ í•„í„°ë§ -> ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ (ì¡°ê±´ë¶€) -> ìƒì„¸ ë‚´ìš©ì— ëŒ€í•´ AI ì˜ˆì¸¡ -> ìµœì¢… í•„í„°ë§

                     # ì´ í˜ì´ì§€ì˜ ìµœê·¼ ê¸°ì‚¬ ëª©ë¡ì— ëŒ€í•´ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ ì‹œë„ (ì´ë¯¸ í¬ë¡¤ë§ëœ URLì€ ê±´ë„ˆë›°ì§€ ì•ŠìŒ)
                    articles_with_details = []
                    for article_summary in articles_to_process:
                        url = article_summary.get('url')
                        # **ìˆ˜ì •:** ì´ë¯¸ í¬ë¡¤ë§ëœ URLë„ ë‹¤ì‹œ ìƒì„¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ë„ë¡ ë³€ê²½ (í•­ìƒ ìµœì‹  ë³¸ë¬¸ í™•ë³´)
                        # if url and url not in self.crawled_urls:
                        if url:
                            print(f"[Electimes] ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ëŠ” ì¤‘: {article_summary.get('title', 'ì œëª© ì—†ìŒ')}")
                            article_details = self.get_article_content(url) # ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§
                            if article_details and article_details.get('content'):
                                # ìƒì„¸ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°, ê¸°ì¡´ ëª©ë¡ ì •ë³´ì— í•©ì¹¨
                                full_article = {**article_summary, **article_details}
                                articles_with_details.append(full_article)
                                print(f"[Electimes] ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ ì™„ë£Œ: {article_summary.get('title', 'ì œëª© ì—†ìŒ')}")
                            else:
                                print(f"[Electimes] ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš© ì—†ìŒ: {article_summary.get('title', 'ì œëª© ì—†ìŒ')}")
                        else:
                             print(f"[Electimes] URL ì •ë³´ê°€ ì—†ì–´ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ ìŠ¤í‚µ: {article_summary.get('title', 'ì œëª© ì—†ìŒ')}") # Debug log ì œê±°

                    # ìƒì„¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¨ ê¸°ì‚¬ë“¤ì— ëŒ€í•´ AI ì¶”ì²œ ì˜ˆì¸¡
                    articles_after_ai_predict = []
                    if articles_with_details and self.ai_recommender and self.vectorizer:
                        # í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (ì œëª© + ë³¸ë¬¸)
                        texts = [f"{a.get('title', '')} {a.get('content', '')}" for a in articles_with_details]

                        # ë²¡í„°í™” ë° ì˜ˆì¸¡
                        try:
                            X_vec = self.vectorizer.transform(texts)
                            # Use the loaded model's predict method directly
                            preds = self.ai_recommender.predict(X_vec)

                            for i, article in enumerate(articles_with_details):
                                article['ai_recommend'] = bool(preds[i])
                                articles_after_ai_predict.append(article)
                            print(f"[Electimes] AI ì¶”ì²œ ì˜ˆì¸¡ ì™„ë£Œ ({len(articles_after_ai_predict)}ê±´)")
                        except Exception as predict_e:
                            print(f"[Electimes] AI ì¶”ì²œ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(predict_e)}")
                            print(traceback.format_exc())
                            # ì˜ˆì¸¡ ì‹¤íŒ¨ ì‹œ AI ì¶”ì²œì€ ê¸°ë³¸ê°’(False) ìœ ì§€
                            articles_after_ai_predict = articles_with_details # ì˜¤ë¥˜ ë°œìƒí•´ë„ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ë„ë¡
                    else:
                        # AI ëª¨ë¸ ì—†ê±°ë‚˜ ì˜ˆì¸¡ ëŒ€ìƒ ì—†ìœ¼ë©´ AI ì¶”ì²œ í•„í„°ë§ ê±´ë„ˆë›°ê³  ë‹¤ìŒ ë‹¨ê³„ë¡œ
                        articles_after_ai_predict = articles_with_details
                        if not (self.ai_recommender and self.vectorizer):
                             print("[Electimes] AI ì¶”ì²œ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ì˜ˆì¸¡ì„ ê±´ë„ˆëœ ìŠµë‹ˆë‹¤.")

                    # 3. í‚¤ì›Œë“œ ë° AI ì¶”ì²œ í•„í„°ë§ (ìµœì¢… ëŒ€ìƒ ì„ ì •)
                    final_articles_to_sync = []
                    for article in articles_after_ai_predict:
                        # í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸ ë° ë§¤ì¹­ëœ í‚¤ì›Œë“œ ì¶”ì¶œ (ë³¸ë¬¸ í¬í•¨)
                        title_and_content = f"{article.get('title', '')} {article.get('content', '')}"
                        contains_kw, matched_keywords = self.contains_keywords_and_extract(title_and_content)
                        
                        # ë§¤ì¹­ëœ í‚¤ì›Œë“œë¥¼ ê¸°ì‚¬ì— ì €ì¥
                        if matched_keywords:
                            article['keywords'] = matched_keywords
                            print(f"[Electimes] í‚¤ì›Œë“œ ë§¤ì¹­: {article.get('title', 'ì œëª© ì—†ìŒ')} â†’ {matched_keywords}")

                        # AI ì¶”ì²œ ê²°ê³¼ í™•ì¸ (AI ëª¨ë¸ ë¡œë“œëœ ê²½ìš°ì—ë§Œ í•„í„°ë§ ì ìš©)
                        passes_ai_filter = True # ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼ (AI ëª¨ë¸ ì—†ëŠ” ê²½ìš°)
                        if self.ai_recommender and self.vectorizer:
                             # AI ì¶”ì²œ ê²°ê³¼ê°€ Trueì¸ ê¸°ì‚¬ë§Œ í†µê³¼
                             passes_ai_filter = article.get('ai_recommend', False)
                             # print(f"[Electimes] ê¸°ì‚¬ '{article.get('title', '')}': í‚¤ì›Œë“œ={contains_kw}, AIì¶”ì²œ={article.get('ai_recommend', False)}, í†µê³¼={contains_kw and passes_ai_filter}") # Debug log ì œê±°

                        # ìµœì¢… í•„í„°ë§: í‚¤ì›Œë“œ í¬í•¨ OR AI ì¶”ì²œ True
                        # í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜ AIê°€ ì¶”ì²œí•œ ê²½ìš° í¬ë¡¤ë§ ëŒ€ìƒ
                        if contains_kw or (self.ai_recommender and article.get('ai_recommend', False)):
                            final_articles_to_sync.append(article)
                            # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê¸°ì‚¬ URL ì €ì¥
                            self.save_crawled_url(article.get('url'))
                            
                            # í¬ë¡¤ë§ ì´ìœ  í‘œì‹œ
                            reason = []
                            if contains_kw:
                                reason.append(f"í‚¤ì›Œë“œ: {matched_keywords}")
                            if article.get('ai_recommend', False):
                                reason.append("AIì¶”ì²œ")
                            print(f"[Electimes] ìµœì¢… í¬ë¡¤ë§ ëŒ€ìƒ: {article.get('title', 'ì œëª© ì—†ìŒ')} (ì´ìœ : {', '.join(reason)})")
                        # else:
                        #      print(f"[Electimes] ìµœì¢… ì œì™¸: {article.get('title', 'ì œëª© ì—†ìŒ')} (í‚¤ì›Œë“œ: {contains_kw}, AIì¶”ì²œ í†µê³¼: {passes_ai_filter}))") # Debug log ì œê±°

                    crawled_articles_details.extend(final_articles_to_sync)

                except Exception as process_e:
                    print(f"[Electimes] í˜ì´ì§€ {page} ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(process_e)}")
                    print(traceback.format_exc())
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ í˜ì´ì§€ì˜ ë‚˜ë¨¸ì§€ ê¸°ì‚¬ëŠ” ê±´ë„ˆë›¸ ìˆ˜ ìˆìŒ (êµ¬í˜„ì— ë”°ë¼ ë‹¤ë¦„)

            page += 1 # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™

        print(f"[Electimes] í¬ë¡¤ë§ ì¢…ë£Œ. ì´ {len(crawled_articles_details)}ê±´ì˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ.")
        return crawled_articles_details

    def get_article_content(self, url: str) -> Dict[str, Any]:
        """ğŸ”§ ê°œì„ ëœ ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ìŠ¤ë§ˆíŠ¸ ì¬ì‹œë„ ì ìš©)"""
        
        # Extract idxno from URL for debug filename
        match_idxno = re.search(r'idxno=(\d+)', url)
        idxno = match_idxno.group(1) if match_idxno else 'unknown'
        debug_file = f"debug_article_{idxno}.html"

        def fetch_article():
            """ì‹¤ì œ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° ì‘ì—…"""
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'

            # Save HTML content to file for debugging
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            print(f"Saved HTML content to {debug_file}")

            return response.text

        # ğŸš€ ìŠ¤ë§ˆíŠ¸ ì¬ì‹œë„ ì‹œìŠ¤í…œ ì‚¬ìš©
        html_content = self._smart_retry(
            operation_name=f"ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ({url})",
            operation_func=fetch_article,
            max_retries=3,
            base_delay=2.0
        )

        if not html_content:
            print(f"[Electimes] âŒ ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ìµœì¢… ì‹¤íŒ¨: {url}")
            return {'content': '', 'attachments': [], 'published_date': None}

        # Parse HTML content
        soup = BeautifulSoup(html_content, 'html.parser')
        published_date = None

        # 1. Try to find date in article header
        article_header = soup.select_one('.article-header')
        if article_header:
            date_text = article_header.get_text()
            print(f"Found article header text: {date_text}")
            date_match = re.search(r'(\d{4}\.\d{2}\.\d{2})', date_text)
            if date_match:
                # ğŸ”§ ì•ˆì „í•œ ë‚ ì§œ íŒŒì‹± ë©”ì„œë“œ ì‚¬ìš©
                published_date = self._parse_date_safely(date_match.group(1))
                if published_date:
                    print(f"Extracted date from article header: {published_date}")
                else:
                    print(f"Error parsing article header date: {date_match.group(1)}")

        # 2. Try meta tag if not found in header
        if not published_date:
            meta_time = soup.find('meta', {'property': 'article:published_time'})
            if meta_time and meta_time.get('content'):
                # ğŸ”§ ì•ˆì „í•œ ë‚ ì§œ íŒŒì‹±ìœ¼ë¡œ ê°œì„  
                date_str = meta_time['content'][:10]
                print(f"Found meta date string: {date_str}")
                
                # ì—°ë„ ì¡°ì • í›„ íŒŒì‹± ì‹œë„
                date_parts = date_str.split('-')
                if len(date_parts) == 3:
                    current_year = datetime.now().year
                    adjusted_date_str = f"{current_year}-{date_parts[1]}-{date_parts[2]}"
                    published_date = self._parse_date_safely(adjusted_date_str)
                    if published_date:
                        print(f"Extracted date from meta (adjusted year): {published_date}")
                    else:
                        print(f"Error parsing meta published_time: {adjusted_date_str}")

        # 3. Fallback: <li>ì…ë ¥ YYYY.MM.DD HH:MM</li>
        if not published_date:
            li_input = soup.find('li', string=re.compile(r'ì…ë ¥'))
            if li_input:
                print(f"Found input li text: {li_input.text}")
                date_match = re.search(r'(\d{4}\.\d{2}\.\d{2})', li_input.text)
                if date_match:
                    # ğŸ”§ ì•ˆì „í•œ ë‚ ì§œ íŒŒì‹± ë©”ì„œë“œ ì‚¬ìš©
                    published_date = self._parse_date_safely(date_match.group(1))
                    if published_date:
                        print(f"Extracted date from <li>: {published_date}")
                    else:
                        print(f"Error parsing <li> date: {date_match.group(1)}")

        if not published_date:
            print("Date not found in any source.")

        # Extract content
        content = ''
        content_selectors = ['.view-cont', '.article-content', '.article-body', '#article-view-content-div', '.content']
        for selector in content_selectors:
            content_element = soup.select_one(selector)
            if content_element and content_element.text.strip():
                content = content_element.text.strip()
                break
        if not content or len(content.strip()) < 10:
            content = 'ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨'
        print(f"Extracted content length: {len(content)}")
        if len(content) > 100:
            print(f"Content preview: {content[:100]}...")

        print(f"Successfully processed article: {url}")
        return {
            'content': content,
            'attachments': [],
            'published_date': published_date
        } 