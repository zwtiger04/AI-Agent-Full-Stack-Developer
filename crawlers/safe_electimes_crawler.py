#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”§ Phase 2: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ê°€ ì ìš©ëœ ì•ˆì „í•œ ElectimesCrawler

**ì£¼ìš” ê°œì„ ì‚¬í•­**:
- Context Manager íŒ¨í„´ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ìë™ ê´€ë¦¬
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬
- ì˜ˆì™¸ ì•ˆì „ì„± ë³´ì¥
- ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ ë°©ì§€
"""

from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Iterator
from bs4 import BeautifulSoup
import requests
import json
import os
import re
import time
import random
import pytz
import gc
from contextlib import contextmanager

# Phase 1 ê°œì„ ì‚¬í•­ ì„í¬íŠ¸
try:
    from crawlers.resource_managers import ResourceMonitor, SessionManager
except ImportError:
    print("âš ï¸ resource_managers ì„í¬íŠ¸ ì‹¤íŒ¨ - ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ì‚¬ìš©")
    ResourceMonitor = None
    SessionManager = None

# ê¸°ì¡´ imports
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.common.exceptions import TimeoutException
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

try:
    from crawlers.base_crawler import BaseCrawler
    from recommenders.article_recommender import ArticleRecommender
    from notion.notion_client import NotionClient
    import joblib
    from processors.keyword_processor import KeywordProcessor
    from ai_update_content import clean_article_content, generate_one_line_summary_with_llm, generate_key_content
    FULL_DEPENDENCIES = True
except ImportError:
    FULL_DEPENDENCIES = False
    BaseCrawler = object


class SafeElectimesCrawler(BaseCrawler if FULL_DEPENDENCIES else object):
    """
    ğŸ”§ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ê°€ ì ìš©ëœ ì•ˆì „í•œ ì „ê¸°ì‹ ë¬¸ í¬ë¡¤ëŸ¬
    
    **Context Manager íŒ¨í„´ ì ìš©**:
    - with ë¬¸ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ìë™ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    - ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ì•ˆì „í•œ ì •ë¦¬ ë³´ì¥
    - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ì œí•œ
    """
    
    # ì „ë ¥ ì‚°ì—… ê´€ë ¨ í‚¤ì›Œë“œ
    KEYWORDS = [
        'ì¬ìƒì—ë„ˆì§€', 'ì „ë ¥ì¤‘ê°œì‚¬ì—…', 'VPP', 'ì „ë ¥ì‹œì¥', 'ESS', 
        'ì¶œë ¥ì œì–´', 'ì¤‘ì•™ê³„ì•½', 'ì €íƒ„ì†Œ ìš©ëŸ‰', 
        'ì¬ìƒì—ë„ˆì§€ì…ì°°', 'ë³´ì¡°ì„œë¹„ìŠ¤', 
        'ì˜ˆë¹„ë ¥ì‹œì¥', 'í•˜í–¥ì˜ˆë¹„ë ¥', 'ê³„í†µí¬í™”',
        'ì „ë ¥ë§', 'ê¸°í›„ì—ë„ˆì§€ë¶€', 'íƒœì–‘ê´‘', 'ì „ë ¥ê°ë…ì›'
    ]
    
    def __init__(self, notion_client=None, recommender=None, 
                 batch_size: int = 50, memory_limit_mb: float = 100):
        """
        ì´ˆê¸°í™” (Context Managerë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”ëŠ” __enter__ì—ì„œ)
        """
        if FULL_DEPENDENCIES:
            super().__init__('ì „ê¸°ì‹ ë¬¸', 'https://www.electimes.com')
        
        self.base_url = 'https://www.electimes.com'
        self.list_url = f"{self.base_url}/news/articleList.html?view_type=sm"
        self._source_name = 'ì „ê¸°ì‹ ë¬¸'
        self.batch_size = batch_size
        self.memory_limit_mb = memory_limit_mb
        
        # í¬ë¡¤ë§ ì´ë ¥
        self.history_file = 'crawled_articles.json'
        self.crawled_urls = set()
        
        # ë¦¬ì†ŒìŠ¤ë“¤ (Context Managerì—ì„œ ê´€ë¦¬)
        self.session = None
        self.driver = None
        self.resource_monitor = None
        
        # AI ê´€ë ¨ (ì„ íƒì )
        self.notion_client = notion_client
        self.ai_recommender = recommender
        self.vectorizer = None
        
        print("SafeElectimesCrawler ì¤€ë¹„ ì™„ë£Œ (Context Managerë¡œ ì‚¬ìš©)")

    def __enter__(self):
        """Context Manager ì§„ì…: ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”"""
        print("ğŸš€ SafeElectimesCrawler ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            # 1. ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
            if ResourceMonitor:
                self.resource_monitor = ResourceMonitor(warning_threshold_mb=self.memory_limit_mb)
                self.resource_monitor.__enter__()
            
            # 2. HTTP Session ì´ˆê¸°í™”
            if SessionManager:
                self.session_manager = SessionManager()
                self.session = self.session_manager.__enter__()
            else:
                # Fallback: ê¸°ë³¸ requests ì‚¬ìš©
                self.session = requests.Session()
                self.session.headers.update({
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
            
            # 3. í¬ë¡¤ë§ ì´ë ¥ ë¡œë“œ
            self.crawled_urls = self.load_crawled_urls()
            
            # 4. AI ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
            self._load_ai_models()
            
            print("âœ… SafeElectimesCrawler ì´ˆê¸°í™” ì™„ë£Œ")
            return self
            
        except Exception as e:
            print(f"âŒ SafeElectimesCrawler ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._cleanup_all()
            raise
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context Manager ì¢…ë£Œ: ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì•ˆì „ ì •ë¦¬"""
        print("ğŸ”„ SafeElectimesCrawler ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        self._cleanup_all()
        
        if exc_type:
            print(f"âš ï¸ ì˜ˆì™¸ì™€ í•¨ê»˜ ì¢…ë£Œ: {exc_type.__name__}: {exc_val}")
        else:
            print("âœ… SafeElectimesCrawler ì •ìƒ ì¢…ë£Œ")
    
    def _cleanup_all(self):
        """ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        
        # Session ì •ë¦¬
        if hasattr(self, 'session_manager') and self.session_manager:
            try:
                self.session_manager.__exit__(None, None, None)
            except Exception as e:
                print(f"âš ï¸ Session ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        elif self.session:
            try:
                self.session.close()
            except Exception as e:
                print(f"âš ï¸ Session ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
        if self.resource_monitor:
            try:
                self.resource_monitor.__exit__(None, None, None)
            except Exception as e:
                print(f"âš ï¸ ResourceMonitor ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ëª…ì‹œì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
    
    def _load_ai_models(self):
        """AI ëª¨ë¸ ë¡œë“œ (ì„ íƒì )"""
        try:
            model_path = 'feedback/ai_recommend_model.joblib'
            vectorizer_path = 'feedback/ai_recommend_vectorizer.joblib'

            if os.path.exists(model_path) and os.path.exists(vectorizer_path):
                if FULL_DEPENDENCIES:
                    self.ai_recommender = joblib.load(model_path)
                    self.vectorizer = joblib.load(vectorizer_path)
                    print("âœ… AI ì¶”ì²œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                else:
                    print("âš ï¸ joblib ì—†ìŒ - AI ëª¨ë¸ ë¡œë“œ ê±´ë„ˆëœ€")
            else:
                print("â„¹ï¸ AI ëª¨ë¸ íŒŒì¼ ì—†ìŒ - ê¸°ë³¸ í¬ë¡¤ë§ë§Œ ìˆ˜í–‰")
        except Exception as e:
            print(f"âš ï¸ AI ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

    def load_crawled_urls(self) -> set:
        """ì´ì „ì— í¬ë¡¤ë§í•œ URL ëª©ë¡ì„ ë¡œë“œ"""
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
            except Exception as e:
                print(f"âš ï¸ í¬ë¡¤ë§ ì´ë ¥ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return set()

    def save_crawled_url(self, url: str):
        """í¬ë¡¤ë§í•œ URLì„ ì•ˆì „í•˜ê²Œ ì €ì¥"""
        self.crawled_urls.add(url)
        try:
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.crawled_urls), f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ í¬ë¡¤ë§ ì´ë ¥ ì €ì¥ ì˜¤ë¥˜: {str(e)}")

    def _parse_date_safely(self, date_str: str) -> Optional[datetime]:
        """ğŸ”§ Phase 1: ì•ˆì „í•œ ë‚ ì§œ íŒŒì‹± ë©”ì„œë“œ"""
        if not date_str or not date_str.strip():
            return None
            
        date_str = date_str.strip()
        
        date_patterns = [
            '%Y.%m.%d %H:%M',     # 2025.06.03 10:00
            '%Y.%m.%d',           # 2025.06.03
            '%Y-%m-%d %H:%M:%S',  # 2025-06-03 10:00:00
            '%Y-%m-%d %H:%M',     # 2025-06-03 10:00
            '%Y-%m-%d',           # 2025-06-03
            '%Y/%m/%d %H:%M',     # 2025/06/03 10:00
            '%Y/%m/%d',           # 2025/06/03
            '%m.%d %H:%M',        # 06.03 10:00 (ì—°ë„ ì—†ìŒ)
            '%m-%d %H:%M',        # 06-03 10:00 (ì—°ë„ ì—†ìŒ)
            '%m/%d %H:%M',        # 06/03 10:00 (ì—°ë„ ì—†ìŒ)
        ]
        
        for pattern in date_patterns:
            try:
                parsed_date = datetime.strptime(date_str, pattern)
                
                if '%Y' not in pattern:
                    current_year = datetime.now().year
                    parsed_date = parsed_date.replace(year=current_year)
                
                return parsed_date
                
            except ValueError:
                continue
        
        return None

    def _smart_retry(self, operation_name: str, operation_func, max_retries: int = 3, base_delay: float = 2.0):
        """ğŸ”§ Phase 1: ìŠ¤ë§ˆíŠ¸ ë„¤íŠ¸ì›Œí¬ ì¬ì‹œë„ ì‹œìŠ¤í…œ"""
        retryable_exceptions = (
            requests.exceptions.ConnectionError,
            requests.exceptions.Timeout,
            requests.exceptions.HTTPError,
        )
        
        for attempt in range(max_retries):
            try:
                result = operation_func()
                return result
                
            except retryable_exceptions as e:
                if attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt)
                    jitter = random.uniform(0.1, 0.3) * delay
                    total_delay = delay + jitter
                    
                    print(f"   âš ï¸ {operation_name} ì¬ì‹œë„ {attempt + 1}/{max_retries}: {total_delay:.1f}ì´ˆ ëŒ€ê¸°")
                    time.sleep(total_delay)
                else:
                    print(f"   âŒ {operation_name} ìµœì¢… ì‹¤íŒ¨: {str(e)}")
                    break
                    
            except Exception as e:
                print(f"   âŒ {operation_name} ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
                break
        
        return None

    def is_recent_article(self, date: datetime) -> bool:
        """ê¸°ì‚¬ê°€ í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ìµœê·¼ 3ì¼ ë‚´ì˜ ê²ƒì¸ì§€ í™•ì¸"""
        kst = pytz.timezone('Asia/Seoul')
        now_kst = datetime.now(kst)
        today_kst = now_kst.date()
        three_days_ago_kst = today_kst - timedelta(days=3)

        article_date = date.date()
        is_recent = article_date >= three_days_ago_kst
        return is_recent

    def contains_keywords(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        found_keywords = [keyword for keyword in self.KEYWORDS if keyword in text]
        return len(found_keywords) > 0

    @contextmanager
    def batch_processor(self, items: List[Any]) -> Iterator[List[Any]]:
        """ğŸ”§ Phase 2: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬"""
        print(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(items)}ê°œ í•­ëª© (ë°°ì¹˜ í¬ê¸°: {self.batch_size})")
        
        try:
            for i in range(0, len(items), self.batch_size):
                batch = items[i:i + self.batch_size]
                
                print(f"   ì²˜ë¦¬ ì¤‘: ë°°ì¹˜ {i // self.batch_size + 1} ({len(batch)}ê°œ í•­ëª©)")
                
                yield batch
                
                # ë°°ì¹˜ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
        
        print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")

    def crawl_safely(self, max_pages: int = 5) -> Iterator[Dict[str, Any]]:
        """
        ğŸ”§ Phase 2: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì•ˆì „ í¬ë¡¤ë§
        
        **íŠ¹ì§•**:
        - ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
        - ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥
        - ì˜ˆì™¸ ì•ˆì „ì„± ë³´ì¥
        
        Args:
            max_pages: ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜
            
        Yields:
            Dict[str, Any]: í¬ë¡¤ë§ëœ ê¸°ì‚¬ ì •ë³´
        """
        print(f"ğŸš€ ì•ˆì „ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        total_articles = 0
        recent_articles = 0
        keyword_matched = 0
        
        for page in range(1, max_pages + 1):
            print(f"ğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
            
            # í˜ì´ì§€ë³„ ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            page_articles = self._fetch_articles_safely(page)
            
            if not page_articles:
                print(f"   í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ ì—†ìŒ - í¬ë¡¤ë§ ì¢…ë£Œ")
                break
    def crawl_safely(self, max_pages: int = 5) -> Iterator[Dict[str, Any]]:
        """
        ğŸ”§ Phase 2: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì•ˆì „ í¬ë¡¤ë§
        """
        print(f"ğŸš€ ì•ˆì „ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        total_articles = 0
        recent_articles = 0
        keyword_matched = 0
        
        for page in range(1, max_pages + 1):
            print(f"ğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
            
            # í˜ì´ì§€ë³„ ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            page_articles = self._fetch_articles_safely(page)
            
            if not page_articles:
                print(f"   í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ ì—†ìŒ - í¬ë¡¤ë§ ì¢…ë£Œ")
                break
            
            total_articles += len(page_articles)
            
            # ê¸°ì‚¬ë³„ ì²˜ë¦¬ (ë°°ì¹˜ ì²˜ë¦¬ ìˆ˜ì •)
            for article in page_articles:
                # ë‚ ì§œ í•„í„°ë§
                if not article.get("published_date") or not self.is_recent_article(article["published_date"]):
                    continue
                
                recent_articles += 1
                
                # í‚¤ì›Œë“œ í•„í„°ë§
                title_and_content = article.get("title", "") + " " + article.get("content", "")
                if not self.contains_keywords(title_and_content):
                    continue
                
                keyword_matched += 1
                
                # URL ì €ì¥
                if article.get("url"):
                    self.save_crawled_url(article["url"])
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ í•˜ë‚˜ì”© yield
                yield article
            
            # í˜ì´ì§€ ì²˜ë¦¬ í›„ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
                        if article.get('url'):
                            self.save_crawled_url(article['url'])
                        
                        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ í•˜ë‚˜ì”© yield
                        yield article
            
            # í˜ì´ì§€ ì²˜ë¦¬ í›„ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
        
        print(f"ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ í†µê³„:")
        print(f"   ì „ì²´ ê¸°ì‚¬: {total_articles}ê°œ")
        print(f"   ìµœê·¼ ê¸°ì‚¬: {recent_articles}ê°œ")
        print(f"   í‚¤ì›Œë“œ ë§¤ì¹­: {keyword_matched}ê°œ")

    def _fetch_articles_safely(self, page: int = 1) -> List[Dict[str, Any]]:
        """ì•ˆì „í•œ ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        url = f"{self.list_url}&page={page}"
        
        def fetch_page():
            if not self.session:
                raise RuntimeError("Sessionì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return response.text

        html_content = self._smart_retry(
            operation_name=f"ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ {page})",
            operation_func=fetch_page,
            max_retries=3,
            base_delay=1.5
        )

        if not html_content:
            return []
            
        return self._parse_articles_from_html(html_content)

    def _parse_articles_from_html(self, html_content: str) -> List[Dict[str, Any]]:
        """HTMLì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        articles = []
        article_items = soup.select('section#section-list li.item')
        
        for item in article_items:
            title_link = item.select_one('h4.titles a.linked')
            date_tag = item.select_one('em.replace-date')
            source_tag = item.select_one('span.byline a')

            title = title_link.text.strip() if title_link else None
            url = self.base_url + title_link['href'] if title_link and title_link.has_attr('href') else None
            date_str = date_tag.text.strip() if date_tag else None
            source = source_tag.text.strip() if source_tag else self._source_name

            # ì•ˆì „í•œ ë‚ ì§œ íŒŒì‹±
            published_date = None
            if date_str:
                published_date = self._parse_date_safely(date_str)
                        
            if title and url and published_date:
                articles.append({
                    'title': title,
                    'url': url,
                    'published_date': published_date,
                    'source': source,
                    'content': '',  # í•„ìš” ì‹œ ë³„ë„ë¡œ ê°€ì ¸ì˜´
                    'keywords': [],
                    'ai_recommend': False
                })
        
        return articles


# í¸ì˜ í•¨ìˆ˜: ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€
def create_safe_crawler(*args, **kwargs):
    """ì•ˆì „í•œ í¬ë¡¤ëŸ¬ ìƒì„± í•¨ìˆ˜"""
    return SafeElectimesCrawler(*args, **kwargs)


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸ§ª SafeElectimesCrawler í…ŒìŠ¤íŠ¸")
    
    # Context Managerë¡œ ì•ˆì „í•˜ê²Œ ì‚¬ìš©
    with SafeElectimesCrawler(batch_size=10) as crawler:
        articles = list(crawler.crawl_safely(max_pages=2))
        print(f"âœ… ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        
        for i, article in enumerate(articles[:3], 1):
            print(f"ğŸ“° ê¸°ì‚¬ {i}: {article['title'][:50]}...")
